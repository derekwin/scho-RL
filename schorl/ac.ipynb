{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs : Box([-1. -1. -8.], [1. 1. 8.], (3,), float32)\n",
      " obs shape : (3,)\n",
      "aciton : Box(-2.0, 2.0, (1,), float32)\n",
      " aciton shape : (1,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JaceL\\miniconda3\\lib\\site-packages\\gym\\core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\JaceL\\miniconda3\\lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "from schorl_utils.envs import *\n",
    "from schorl_utils.functions import Train, Agent\n",
    "from schorl_utils.net import generate_mlpnet, show_net_structure, ContinuousPolicyMlp\n",
    "import gym\n",
    "\n",
    "# 以Pendulum环境为例 连续状态连续动作\n",
    "env = gym.make('Pendulum-v1')\n",
    "print(f\"obs : {env.observation_space}\\n obs shape : {env.observation_space.shape}\")\n",
    "print(f\"aciton : {env.action_space}\\n aciton shape : {env.action_space.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor-Critic：结合价值学习和策略学习的算法\n",
    "Actor：策略网络，用于动作选择<br>\n",
    "Critic：价值网络，给动作打分\n",
    "\n",
    "$V_\\pi(S)=\\sum_a \\pi(a|s) \\cdot Q_\\pi(s,a)$\n",
    "\n",
    "使用神经网络 $\\pi(a|s;\\theta)$ 近似策略 $\\pi(a|s)$ ，$\\theta$为神经网络参数<br>\n",
    "使用神经网络 $q(s,a;w)$ 近似动作价值函数 $Q_\\pi(s,a)$，$w$神经网络参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 连续动作空间 策略网络\n",
    "连续动作空间，policy网络预测一组 均值和std标准差来 描述 每个动作的分布<br>\n",
    "依照此分布采样获得动作和该动作的log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "策略网络\n",
      "ContinuousPolicyMlp(\n",
      "  (mlp): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=3, out_features=128, bias=True)\n",
      "    (2): Tanh()\n",
      "  )\n",
      "  (fc_mean): Linear(in_features=128, out_features=1, bias=True)\n",
      "  (fc_std): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "价值网络\n",
      "The structure of the net:\n",
      "Flatten output shape : torch.Size([5, 4])\n",
      "Linear output shape : torch.Size([5, 128])\n",
      "Tanh output shape : torch.Size([5, 128])\n",
      "Linear output shape : torch.Size([5, 1])\n"
     ]
    }
   ],
   "source": [
    "# 搭建策略网络\n",
    "# 如果是离散动作空间，输出增加一个softmax层，将输出的动作向量变为概率分布；后续可以使用epsilon贪心平衡探索利用\n",
    "# 如果是连续动作空间，可以增加一个tahn层，将输出变为(-1,1)内的值，再根据动作的范围进行放缩，\n",
    "#       后续动作选择的时候通过增加噪声来平衡探索利用\n",
    "policyNet = ContinuousPolicyMlp([env.observation_space.shape[0], \n",
    "                128, env.action_space.shape[0]])\n",
    "\n",
    "# 价值网络\n",
    "# 输入是 状态和actor选择的动作，输出是一个q值，表对决策的评价\n",
    "valueNet = generate_mlpnet(mlp_layers=[env.observation_space.shape[0]+env.action_space.shape[0], 128, 1])\n",
    "\n",
    "print(\"策略网络\")\n",
    "print(policyNet)\n",
    "print(\"价值网络\")\n",
    "show_net_structure(valueNet, (env.observation_space.shape[0]+env.action_space.shape[0],))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练过程\n",
    "1. 观测状态获取状态state\n",
    "2. 根据策略随机采样动作\n",
    "3. 执行动作，获得下一个状态和奖励\n",
    "4. 根据奖励时序差分方法更新价值网络\n",
    "5. 使用策略梯度方法更新策略网络\n",
    "\n",
    "#### TD更新价值网络\n",
    "1. 计算$q(s_t,a_t;w_t)$和$q(s_{t+1},a_{t+1};w_t)$\n",
    "2. TD target: $y_t=r_t + \\gamma \\cdot q(s_{t+1},a_{t+1};w_t)$  所以AC算法是on policy的方法\n",
    "3. loss: $L(W)=1/2(q(s_t,a_t;w_t) - y_t)$\n",
    "\n",
    "#### 策略梯度更新策略网络\n",
    "策略网络根据价值网络的打分进行梯度上升更新<br>\n",
    "即  $-log\\_prob*q_t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACagent(Agent):\n",
    "    def __init__(self, policyNet, valueNet, action_scale, exploration_noise=0.1, device=get_device(), gamma=0.9, lr=0.002, optim=torch.optim.Adam, loss=F.mse_loss, datatype=torch.float) -> None:\n",
    "        self.policy = policyNet.to(device)\n",
    "        self.value = valueNet.to(device)\n",
    "        self.device = device\n",
    "        self.gamma_q = gamma\n",
    "        self.lr = 0.002\n",
    "        self.policy_optim = optim(self.policy.parameters(), lr=lr)\n",
    "        self.value_optim = optim(self.value.parameters(), lr=lr)\n",
    "        self.type = datatype\n",
    "        self.action_scale = action_scale\n",
    "        self.exploration_noise = exploration_noise\n",
    "        self.qloss = loss\n",
    "    \n",
    "    def __call__(self, state:np.ndarray):\n",
    "        state = torch.tensor([state], dtype=self.type).to(self.device)\n",
    "        mean, std = self.policy(state)\n",
    "        dist = torch.distributions.Normal(mean, std)\n",
    "        normal_sample = dist.rsample()  # rsample()是重参数化采样\n",
    "        log_prob = dist.log_prob(normal_sample)\n",
    "        action = torch.tanh(normal_sample)\n",
    "        # 计算tanh_normal分布的对数概率密度\n",
    "        log_prob = log_prob - torch.log(1 - torch.tanh(action).pow(2) + 1e-7)\n",
    "        return action * self.action_scale, log_prob\n",
    "\n",
    "    def update(self, state, action, log_prob, reward, next_state, done):\n",
    "        # 单步更新\n",
    "        state = torch.tensor([state], dtype=self.type).to(self.device)\n",
    "        action = torch.tensor(action, dtype=self.type).to(self.device)\n",
    "        next_state_t = torch.tensor([next_state], dtype=self.type).to(self.device)\n",
    "        reward = torch.tensor(reward, dtype=self.type).to(self.device)\n",
    "\n",
    "        # print(state.shape)    #torch.Size([1, 3])\n",
    "        # print(action.shape)   #torch.Size([1, 1])\n",
    "\n",
    "        Q = self.value(torch.cat((state, action),1))    #价值网络对策略选择的动作进行评价\n",
    "        \n",
    "        # 计算价值梯度，更新价值网络\n",
    "        self.value_optim.zero_grad()\n",
    "\n",
    "        action_next, _ = self.__call__(next_state)  # action, prob\n",
    "        action_next = torch.tensor(action_next, dtype=self.type).to(self.device)\n",
    "        \n",
    "        # print(next_state_t.shape)    #torch.Size([1, 3])\n",
    "        # print(action_next.shape)   #torch.Size([1, 1])\n",
    "\n",
    "        Qnext = self.value(torch.cat((next_state_t, action_next), 1))\n",
    "        y_i = reward + self.gamma_q * Qnext\n",
    "        loss_q = self.qloss(Q, y_i)\n",
    "        loss_q.backward()\n",
    "\n",
    "        self.value_optim.step()\n",
    "        \n",
    "\n",
    "        # 计算策略梯度, 更新策略网络\n",
    "        self.policy_optim.zero_grad()\n",
    "\n",
    "        q = Q.item()\n",
    "        loss_p = -log_prob * q\n",
    "        loss_p.backward()\n",
    "\n",
    "        self.policy_optim.step()\n",
    "\n",
    "        return loss_p, loss_q\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACTrain(Train):\n",
    "    def __init__(self, env, agent, tblogpath) -> None:\n",
    "        super().__init__(env, agent, tblogpath)\n",
    "\n",
    "    def run_episode(self):\n",
    "        reward_list = []\n",
    "        loss_list_p = []\n",
    "        loss_list_q = []\n",
    "\n",
    "        done = False\n",
    "        reward_list = []\n",
    "        prob_list = []\n",
    "        state = self.env.reset()    # 这个场景有问题，reset获得的是 [ 0.31281108  0.9498154  -0.63081366] (3,)\n",
    "        while not done:\n",
    "            action, log_prob = self.agent(state)\n",
    "            action = action.detach().numpy()        # 这个env需要转成numpy才能跑\n",
    "            got = self.env.step(action)     #这里的state是 [[ 0.31505278]\n",
    "                                            #                [ 0.94907415]\n",
    "                                            #                [-0.04722166]] (3, 1)\n",
    "            reward = got[1]\n",
    "            next_state = got[0].transpose()[0]\n",
    "            done = got[2]\n",
    "            # print(\"next\",state, next_state)\n",
    "            loss_policy, loss_value = self.agent.update(state, action, log_prob, reward, next_state, done)\n",
    "            loss_list_p.append(loss_policy.item())\n",
    "            loss_list_q.append(loss_value.item())\n",
    "            reward_list.append(reward)\n",
    "            state = next_state\n",
    "        \n",
    "        self.env.close()\n",
    "        return {'accumulated reward':sum(reward_list), 'policy_loss':np.mean(loss_list_p), 'qvalue_loss':np.mean(loss_list_q)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]C:\\Users\\JaceL\\AppData\\Local\\Temp\\ipykernel_21044\\3737043199.py:16: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:204.)\n",
      "  state = torch.tensor([state], dtype=self.type).to(self.device)\n",
      "c:\\Users\\JaceL\\miniconda3\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "c:\\Users\\JaceL\\miniconda3\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:260: UserWarning: \u001b[33mWARN: The reward returned by `step()` must be a float, int, np.integer or np.floating, actual type: <class 'numpy.ndarray'>\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\JaceL\\AppData\\Local\\Temp\\ipykernel_21044\\3737043199.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  action_next = torch.tensor(action_next, dtype=self.type).to(self.device)\n",
      "100%|██████████| 500/500 [03:57<00:00,  2.11it/s, episode=499]\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 500\n",
    "\n",
    "policyNet = policyNet\n",
    "valueNet = valueNet\n",
    "\n",
    "agent = ACagent(policyNet=policyNet, valueNet=valueNet, action_scale=2)\n",
    "\n",
    "train = ACTrain(env=env, agent=agent, tblogpath='./aclog')\n",
    "\n",
    "train.train(num_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save_net(agent.policy ,'./model/ACPolicyPendulum.pt')\n",
    "# tensorboard --logdir=./tensorlog --port 8123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JaceL\\AppData\\Local\\Temp\\ipykernel_21044\\3737043199.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  action_next = torch.tensor(action_next, dtype=self.type).to(self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1591.1426]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import time\n",
    "\n",
    "# env = gym.make('Pendulum-v1', new_step_api=True)\n",
    "\n",
    "# agent = ACagent(policyNet=policyNet, valueNet=valueNet, action_scale=2)\n",
    "net = policyNet = ContinuousPolicyMlp([env.observation_space.shape[0], \n",
    "                128, env.action_space.shape[0]])\n",
    "model = torch.load('./model/ACPolicyPendulum.pt')\n",
    "net.load_state_dict(model)\n",
    "\n",
    "done = False\n",
    "state = env.reset()\n",
    "reward_list = []\n",
    "while not done:\n",
    "    action, log_prob = agent(state)\n",
    "    action = action.detach().numpy()        # 这个env需要转成numpy才能跑\n",
    "    got = env.step(action)     #这里的state是 [[ 0.31505278]\n",
    "                                    #                [ 0.94907415]\n",
    "                                    #                [-0.04722166]] (3, 1)\n",
    "    reward = got[1]\n",
    "    next_state = got[0].transpose()[0]\n",
    "    done = got[2]\n",
    "    loss_policy, loss_value = agent.update(state, action, log_prob, reward, next_state)\n",
    "    state = next_state\n",
    "    reward_list.append(reward)\n",
    "env.close()\n",
    "\n",
    "print(sum(reward_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b9f5ab073a657fe4cc5417b6d00e97d396a1575f5354cbd9223aaad774df1bf7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
