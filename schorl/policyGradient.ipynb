{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "状态空间 : Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      " obs shape : (4,)\n",
      "动作空间 : Discrete(2)\n",
      " aciton nums : 2\n"
     ]
    }
   ],
   "source": [
    "from schorl_utils.envs import *\n",
    "from schorl_utils.functions import Train, Agent\n",
    "from schorl_utils.net import generate_mlpnet, show_net_structure\n",
    "import gym\n",
    "\n",
    "# 以CartPole-v1环境为例：连续状态，离散动作环境\n",
    "env = gym.make('CartPole-v1', new_step_api=True)\n",
    "print(f\"状态空间 : {env.observation_space}\\n obs shape : {env.observation_space.shape}\")\n",
    "print(f\"动作空间 : {env.action_space}\\n aciton nums : {env.action_space.n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 策略学习算法\n",
    "策略学习算法要学习的是一个策略函数，我们定义一个参数为$\\theta$的策略函数，\n",
    "\n",
    "该函数的输入状态，输出是动作概率分布。\n",
    "\n",
    "算法在训练过程中追求的目标是寻找一个最优的策略，即一组参数使得策略在当前环境中的回报期望最大化。\n",
    "\n",
    "我们用状态值函数来表征某状态下当前策略的好坏程度，那么策略在环境中的回报期望就是对状态值函数的期望：\n",
    "\n",
    "$\\pi(s)=$\n",
    "$J(\\theta)=\\mathbb{E}[V^{\\pi_{\\theta}}(s_0)]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 下面创建策略网络\n",
    "如果是离散动作空间，输出增加一个softmax层，将输出的动作向量变为概率分布；后续可以使用epsilon贪心策略来平衡探索利用问题\n",
    "\n",
    "如果是连续动作空间，可以增加一个tahn层，将输出变为(-1,1)内的值，再根据动作的范围进行放缩\n",
    "\n",
    "连续动作在做动作选择的时候可以通过增加噪声来平衡探索利用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "策略网络\n",
      "The structure of the net:\n",
      "Flatten output shape : torch.Size([5, 4])\n",
      "Linear output shape : torch.Size([5, 128])\n",
      "Tanh output shape : torch.Size([5, 128])\n",
      "Linear output shape : torch.Size([5, 64])\n",
      "Tanh output shape : torch.Size([5, 64])\n",
      "Linear output shape : torch.Size([5, 2])\n",
      "Softmax output shape : torch.Size([5, 2])\n"
     ]
    }
   ],
   "source": [
    "# 创建一个全连接网络作为策略网络，此处离散动作空间，故增加一个softmax层\n",
    "policyNet = generate_mlpnet([env.observation_space.shape[0], \n",
    "                128, 64, env.action_space.n],\n",
    "                if_softmax=True)\n",
    "\n",
    "print(\"策略网络\")\n",
    "show_net_structure(policyNet, (env.observation_space.shape[0],))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "回顾一下动作价值函数和状态价值函数\n",
    "#### 动作价值函数 Action-value function\n",
    "折扣奖励<br>\n",
    "$U_t=R_t+\\gamma \\cdot R_{t+1} + \\gamma^2 \\cdot R_{t+2} + \\gamma^3 \\cdot \\R_{t+3} + ... $\n",
    "\n",
    "动作值函数<br>\n",
    "$Q_\\pi(s_t,a_t)=\\mathbb{E}[U_t|S_t=s_t, A_t=a_t]$\n",
    "> 动作值函数，是当前状态st下选择动作at的期望回报\n",
    "\n",
    "#### 状态价值函数\n",
    "$V_\\pi(s_t)=\\mathbb{E}[Q_\\pi(s_t,A)]=\\sum_a\\pi(a|s_t) \\cdot Q_\\pi(s_t,a)$\n",
    "> 状态值函数，是当前状态下该策略的期望回报，状态值函数是动作值函数关于策略的所有动作的期望"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 使用神经网络近似策略函数\n",
    "则此时的状态值函数为：<br>\n",
    "$V_\\pi(s_t;\\theta)=\\sum_a\\pi(a|s_t;\\theta) \\cdot Q_\\pi(s_t,a)$\n",
    "\n",
    "定义$J(\\theta)$为对策略网络的评价：<br>\n",
    "$J(\\theta)=\\mathbb{E}[V_\\pi(s_t;\\theta)]$\n",
    "\n",
    "### 采用梯度上升来更新$\\theta$以提升策略\n",
    "我们对$J(\\theta)$求导，但是计算$J(\\theta)$是不切实际的，我们无法遍历所有情况也不会遍历所有情况，所以这个期望是不可能准确算出来。这里采用对$V_\\pi(s;\\theta)$的导数来近似$J(\\theta)$的导数。由于s可以看作是随机采样的其中一个状态，故此时梯度是随机梯度。\n",
    "\n",
    "离散动作：$\\frac{\\partial V(s;\\theta)}{\\partial \\theta} = \\sum_a \n",
    "    \\frac{\\partial \\pi(a|s;\\theta)}{\\partial \\theta} \\cdot Q_\\pi(s,a)$\n",
    "\n",
    "连续动作：$\\frac{\\partial V(s;\\theta)}{\\partial \\theta} = \\mathbb{E}_{A\\sim\\pi(\\cdot|s;\\theta)}\n",
    "    [\\frac{\\partial log \\pi(A|s;\\theta)}{\\partial \\theta} \\cdot Q_\\pi(s,A)]$\n",
    "\n",
    "由于连续动作时，无法准确积分求真实的期望，所以采用蒙特卡洛采样来近似期望。随机采样一组或者多组的动作来计算这个期望。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "策略梯度类算法的大致过程\n",
    "1. 获得状态$s_t$\n",
    "2. 根据策略$\\pi(\\cdot|s_t;\\theta_t)$随机采样动作$a_t$\n",
    "3. 计算$Q(s_t, a_t)$\n",
    "4. 对网络求导(神经网络框架自带功能)，$d_{\\theta,t}=\\frac{\\partial log\\pi(a_t|s_t;\\theta)}{\\partial \\theta}|_{\\theta=\\theta_t}$\n",
    "5. 近似计算策略梯度，$g(a_t,\\theta_t)=Q(s_t,a_t)\\cdot d_{\\theta,t}$\n",
    "6. 更新策略网络，$\\theta_{t+1} = \\theta_t + \\beta \\cdot g(a_t,\\theta_t)$ , $\\beta$学习率\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对Q(s_t,a_t)的计算方式不同，主要有以下两类算法\n",
    "#### 一、REINFORCE算法\n",
    "1. 跑完一局游戏，采样获得一个轨迹，$s_1,a_1,r_1,s_2,a_2,r_2,...$\n",
    "2. 计算累计折扣奖励，$u_t = \\sum^T_{k=t}\\gamma^k r_k$\n",
    "3. Q(s_t,a_t)严格意义上是累计折扣奖励的期望，此处用一局的累计折扣近似期望\n",
    "\n",
    "reinforce算法过程：<br>\n",
    "- 采样一条轨迹\n",
    "- for step in reversed(轨迹):<br>\n",
    "-   - 计算$u_{step}$\n",
    "-   - 更新参数，（$\\theta$+求对数后的导数*对应的Q值）\n",
    "\n",
    "由于计算的Q可能波动较大，一般减去一个base来使之稳定一点，比如V(s),减去Q的均值也可以。\n",
    "\n",
    "#### 二、AC算法\n",
    "用神经网络来近似Q网络，计算q值。<br>\n",
    "好处是算法可以单步执行了，而且随着训练进行神经网络估计得q值比采样得方式要稳定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from schorl_utils.functions import UPDATE_MODE\n",
    "# 此处以reinforce算法实现\n",
    "\n",
    "class Reinforce(Agent):\n",
    "    def __init__(self, \n",
    "            net, \n",
    "            nums_action\n",
    "            ) -> None:\n",
    "        super().__init__()  # 初始化默认参数\n",
    "        self.policy = net\n",
    "        self.nums_action = nums_action\n",
    "        self.policy_optim = self.optim(self.policy.parameters(), self.lr)\n",
    "    \n",
    "    def get_action(self, state:torch.tensor):\n",
    "        # 数据格式转换的操作在Train内部已经实现\n",
    "        probs = self.policy(state)  # 此处返回的是各个动作相应的softmax后的概率组合\n",
    "        action_dist = torch.distributions.Categorical(probs)  # 按概率分布的采样工具\n",
    "        action = action_dist.sample().item()\n",
    "        return action, probs[0,action] # 采样一个动作,后续要用该动作的概率\n",
    "        # 也可以直接这里返回 action_dist.log_prob(action) 即对数概率\n",
    "\n",
    "    def update(self, obs_track:list, *args):\n",
    "        # reinforce算法 蒙特卡洛积分，用ut代替Q\n",
    "        # Q = sum([reward_track[i]*self.gamma**i for i in range(len(reward_track))])\n",
    "\n",
    "        probs_track = args[0]\n",
    "        reward_track = [i.reward for i in obs_track]\n",
    "\n",
    "        self.policy_optim.zero_grad()\n",
    "        # 对每步分别进行loss反向传播累计\n",
    "        for step in range(len(reward_track)):\n",
    "            this_step = reward_track[step:]\n",
    "            Q = sum([this_step[i]*self.gamma**i for i in range(len(this_step))])\n",
    "            log_prob = torch.log(probs_track[step][0])\n",
    "            loss = -log_prob * Q\n",
    "            # 在王树森老师的书中，此处的Q是整场游戏的Q\n",
    "            # 整场的Q，算法学习效果较差，学习慢，数据利用率低\n",
    "            # 在张伟楠老师的代码中，此处的Q是每一步的累计Q \n",
    "            loss.backward()\n",
    "        self.policy_optim.step()\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReinfroceCartpoleTrain(Train):\n",
    "    def __init__(self, env, agent:Reinforce) -> None:\n",
    "        super().__init__(env, agent)\n",
    "        self.update_mode = UPDATE_MODE.MULTI_STEP   # reinforce是多步更新\n",
    "    \n",
    "    def run_episode(self):\n",
    "        done = False\n",
    "        reward_list = []\n",
    "        prob_list = []\n",
    "        state = self.env.reset()\n",
    "        while not done:\n",
    "            action, prob = self.agent(state)\n",
    "            got = self.env.step(action)\n",
    "            reward_list.append(got[1])\n",
    "            prob_list.append(prob)\n",
    "            state = got[0]\n",
    "            done = got[2]\n",
    "        \n",
    "        self.env.close()\n",
    "        loss = self.agent.update(reward_list, prob_list)\n",
    "\n",
    "        return {'accumulated reward':sum(reward_list), 'loss':loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 0:   0%|          | 0/50 [00:00<?, ?it/s]f:\\coding\\scho-rl\\schorl_utils\\functions.py:114: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:204.)\n",
      "  return torch.tensor(data, dtype=self.data_type).to(device=self.device)\n",
      "Iteration 0: 100%|██████████| 50/50 [00:00<00:00, 95.06it/s, episode=50, mean reward=1.000] \n",
      "Iteration 1: 100%|██████████| 50/50 [00:02<00:00, 18.85it/s, episode=100, mean reward=1.000]\n",
      "Iteration 2: 100%|██████████| 50/50 [00:05<00:00,  9.16it/s, episode=150, mean reward=1.000]\n",
      "Iteration 3: 100%|██████████| 50/50 [00:25<00:00,  1.92it/s, episode=200, mean reward=1.000]\n",
      "Iteration 4: 100%|██████████| 50/50 [00:19<00:00,  2.60it/s, episode=250, mean reward=1.000]\n",
      "Iteration 5: 100%|██████████| 50/50 [00:17<00:00,  2.87it/s, episode=300, mean reward=1.000]\n",
      "Iteration 6: 100%|██████████| 50/50 [00:15<00:00,  3.20it/s, episode=350, mean reward=1.000]\n",
      "Iteration 7: 100%|██████████| 50/50 [00:03<00:00, 16.56it/s, episode=400, mean reward=1.000]\n",
      "Iteration 8: 100%|██████████| 50/50 [00:02<00:00, 23.93it/s, episode=450, mean reward=1.000]\n",
      "Iteration 9: 100%|██████████| 50/50 [00:02<00:00, 18.89it/s, episode=500, mean reward=1.000]\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 500\n",
    "\n",
    "net = policyNet\n",
    "\n",
    "agent = Reinforce(net, env.action_space.n)\n",
    "\n",
    "train = ReinfroceCartpoleTrain(env=env, agent=agent)\n",
    "train.tblogpath = './reforcelog'\n",
    "\n",
    "train.train(num_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save_net(agent.policy ,'./model/reforceCartpole.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=./reforcelog --port 8123\n",
    "# open web browser and visit 127.0.0.1:8123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v1', new_step_api=True)\n",
    "\n",
    "net = generate_mlpnet([env.observation_space.shape[0], \n",
    "                128, 64, env.action_space.n],\n",
    "                if_softmax=True)\n",
    "model = torch.load('./model/reforceCartpole.pt')\n",
    "net.load_state_dict(model)\n",
    "\n",
    "done = False\n",
    "state = env.reset()\n",
    "while not done:\n",
    "    env.render()\n",
    "    state = torch.tensor(np.array([state]), dtype=torch.float).to(get_device())\n",
    "    action = net(state).argmax().item()\n",
    "    got = env.step(action=action)\n",
    "    state = got[0]\n",
    "    done = got[2]\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 策略梯度算法进阶\n",
    "\n",
    "引入 baseline 提升算法稳定性：在梯度更新时，将Q换成(Q-b)，其中b可以是状态价值函数，即使用 Q - (Q的期望)来减小随机采样不确定性带来的收敛差的问题。\n",
    "\n",
    "$\\nabla_{\\theta}J(\\theta) = \\mathbb{E}_s[\\mathbb{E}_{A\\sim\\pi(\\cdot|s;\\theta)}\n",
    "    [\\frac{\\partial log \\pi(A|s;\\theta)}{\\partial \\theta} \\cdot (Q_\\pi(s,A) - b)]]$\n",
    "\n",
    "理论可以证明，减去基线的随机梯度是无偏的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 对 REINFORCE算法\n",
    "增加一个神经网络来估计V作为baseline<br>\\\n",
    "注意这里和AC算法的不同，这里的这个神经网络只是作为baseline并没有对策略做评价，这一点和AC算法本质不同。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b9f5ab073a657fe4cc5417b6d00e97d396a1575f5354cbd9223aaad774df1bf7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
