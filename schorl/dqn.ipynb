{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于值函数的算法\n",
    "值函数算法隐式的学习策略。值函数算法的目标是最大化动作值函数：\n",
    "\n",
    "$J(\\theta)=\\mathbb{E}[V^{\\pi_{\\theta}}(s_0)]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all you want\n",
    "from schorl_utils.envs import *\n",
    "from schorl_utils.envs import get_device\n",
    "from schorl_utils.buffer import ReplayBuffer, replaybatch\n",
    "from schorl_utils.net import generate_mlpnet, show_net_structure\n",
    "from schorl_utils.functions import Agent, Train\n",
    "\n",
    "class DQN(Agent):\n",
    "    \"\"\"\n",
    "    The input of the DQN is a continuous state space and the output is a discrete action.   \n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "            net,\n",
    "            nums_action,\n",
    "            device = get_device(),\n",
    "            gamma = 0.9,\n",
    "            epsilon = 0.1,\n",
    "            lr = 2e-3,\n",
    "            optim = torch.optim.Adam,\n",
    "            loss = torch.nn.functional.mse_loss,\n",
    "            datatype = torch.float,\n",
    "            update_target_every = 5,\n",
    "            ) -> None:\n",
    "\n",
    "        self.update_target_every = update_target_every\n",
    "        self.q_net = net.to(device)\n",
    "        self.target_q_net = net.to(device)\n",
    "        self.device = device\n",
    "        self.gamma = gamma\n",
    "        self.nums_action = nums_action\n",
    "        self.epsilon = epsilon\n",
    "        self.count = 0\n",
    "        self.type = datatype\n",
    "        self.loss = loss\n",
    "        self.optim = optim(self.q_net.parameters(), lr)\n",
    "    \n",
    "    def __call__(self, state:np.array):\n",
    "        \"\"\"\n",
    "        return action\n",
    "        \"\"\"\n",
    "        if random.random() < self.epsilon:\n",
    "            return np.random.randint(self.nums_action)\n",
    "        else:\n",
    "            state = torch.tensor(np.array([state]), dtype=self.type).to(self.device)\n",
    "            return self.q_net(state).argmax().item()\n",
    "    \n",
    "    def update(self, batch:replaybatch):\n",
    "        \"\"\"\n",
    "        return loss if you want to record\n",
    "        \"\"\"\n",
    "        state_batch = torch.tensor(batch.states, dtype=self.type).to(self.device)\n",
    "        action_batch = torch.tensor(batch.actions).view(-1,1).to(self.device)\n",
    "        reward_batch = torch.tensor(batch.rewards, dtype=self.type).view(-1,1).to(self.device)\n",
    "        next_state_batch = torch.tensor(batch.next_states, dtype=self.type).to(self.device)\n",
    "        done_batch = torch.tensor(batch.dones, dtype=self.type).view(-1,1).to(self.device)\n",
    "\n",
    "        thisQ = self.q_net(state_batch).gather(1, action_batch) # get q value\n",
    "        next_max = self.target_q_net(next_state_batch).max(1)[0].view(-1, 1)\n",
    "        nextQ = reward_batch + self.gamma * next_max * ( 1 - done_batch)\n",
    "\n",
    "        self.optim.zero_grad()\n",
    "        batch_loss = torch.mean(self.loss(thisQ, nextQ))\n",
    "        batch_loss.backward()\n",
    "        self.optim.step()\n",
    "\n",
    "        if self.count % self.update_target_every == 0:\n",
    "            self.target_q_net.load_state_dict(self.q_net.state_dict())\n",
    "        self.count+=1\n",
    "        \n",
    "        return batch_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DqnTrain(Train):\n",
    "    \n",
    "    def __init__(self, env, agent, replaybuffer, batchsize, tblogpath) -> None:\n",
    "        super().__init__(env=env, agent=agent, tblogpath=tblogpath)\n",
    "        self.replaybuffer = replaybuffer\n",
    "        self.batchsize = batchsize\n",
    "\n",
    "    def run_episode(self):\n",
    "        \"\"\"\n",
    "        rewrite this function to achieve new env interact\n",
    "        Default:\n",
    "            default is dqn run in CartPole-v1\n",
    "        Return:\n",
    "            {'item', itemvalue}\n",
    "        \"\"\"\n",
    "        done = False\n",
    "        state = self.env.reset()\n",
    "        accumulated_reward = 0\n",
    "        step = 1\n",
    "        accumulated_loss = 0\n",
    "        while not done:\n",
    "            action = self.agent(state)\n",
    "            next_state,reward,done, *d = self.env.step(action)\n",
    "            self.replaybuffer.put(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            accumulated_reward += reward\n",
    "            if self.replaybuffer.__len__() > self.batchsize:\n",
    "                replaybatch  = self.replaybuffer.sample(self.batchsize)\n",
    "                loss = self.agent.update(replaybatch)\n",
    "                accumulated_loss += loss\n",
    "                step += 1\n",
    "        self.env.close()    \n",
    "        return {'accumulated_reward': accumulated_reward, 'loss_mean': accumulated_loss/step}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JaceL\\miniconda3\\lib\\site-packages\\gym\\core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\JaceL\\miniconda3\\lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "  7%|▋         | 72/1000 [00:03<00:46, 19.75it/s, episode=71]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mf:\\Coding\\scho-RL\\schorl\\dqn.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 29>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Coding/scho-RL/schorl/dqn.ipynb#W3sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m dqnagent \u001b[39m=\u001b[39m DQN(\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Coding/scho-RL/schorl/dqn.ipynb#W3sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     net \u001b[39m=\u001b[39m net, \n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Coding/scho-RL/schorl/dqn.ipynb#W3sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmse_loss,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Coding/scho-RL/schorl/dqn.ipynb#W3sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     nums_action \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mn\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Coding/scho-RL/schorl/dqn.ipynb#W3sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     )\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Coding/scho-RL/schorl/dqn.ipynb#W3sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m train \u001b[39m=\u001b[39m DqnTrain(\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Coding/scho-RL/schorl/dqn.ipynb#W3sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     env\u001b[39m=\u001b[39menv,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Coding/scho-RL/schorl/dqn.ipynb#W3sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     agent\u001b[39m=\u001b[39mdqnagent,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Coding/scho-RL/schorl/dqn.ipynb#W3sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     tblogpath\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m./tensorlog\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Coding/scho-RL/schorl/dqn.ipynb#W3sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     )\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/Coding/scho-RL/schorl/dqn.ipynb#W3sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m train\u001b[39m.\u001b[39;49mtrain(num_episodes)\n",
      "File \u001b[1;32mc:\\Users\\JaceL\\miniconda3\\lib\\site-packages\\schorl_utils\\functions.py:87\u001b[0m, in \u001b[0;36mTrain.train\u001b[1;34m(self, nums_episode)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[39mwith\u001b[39;00m tqdm(total\u001b[39m=\u001b[39mnums_episode) \u001b[39mas\u001b[39;00m pbar:\n\u001b[0;32m     85\u001b[0m     \u001b[39mfor\u001b[39;00m episode \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(nums_episode):\n\u001b[1;32m---> 87\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_episode()\n\u001b[0;32m     88\u001b[0m         \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m data:\n\u001b[0;32m     89\u001b[0m             tbwriter\u001b[39m.\u001b[39madd_scalar(item, data[item], episode)\n",
      "\u001b[1;32mf:\\Coding\\scho-RL\\schorl\\dqn.ipynb Cell 4\u001b[0m in \u001b[0;36mDqnTrain.run_episode\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Coding/scho-RL/schorl/dqn.ipynb#W3sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplaybuffer\u001b[39m.\u001b[39m\u001b[39m__len__\u001b[39m() \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatchsize:\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Coding/scho-RL/schorl/dqn.ipynb#W3sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     replaybatch  \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplaybuffer\u001b[39m.\u001b[39msample(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatchsize)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/Coding/scho-RL/schorl/dqn.ipynb#W3sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49magent\u001b[39m.\u001b[39;49mupdate(replaybatch)\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Coding/scho-RL/schorl/dqn.ipynb#W3sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     accumulated_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Coding/scho-RL/schorl/dqn.ipynb#W3sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     step \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[1;32mf:\\Coding\\scho-RL\\schorl\\dqn.ipynb Cell 4\u001b[0m in \u001b[0;36mDQN.update\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Coding/scho-RL/schorl/dqn.ipynb#W3sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m batch_loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss(thisQ, nextQ))\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Coding/scho-RL/schorl/dqn.ipynb#W3sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m batch_loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/Coding/scho-RL/schorl/dqn.ipynb#W3sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptim\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Coding/scho-RL/schorl/dqn.ipynb#W3sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcount \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate_target_every \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Coding/scho-RL/schorl/dqn.ipynb#W3sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_q_net\u001b[39m.\u001b[39mload_state_dict(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_net\u001b[39m.\u001b[39mstate_dict())\n",
      "File \u001b[1;32mc:\\Users\\JaceL\\miniconda3\\lib\\site-packages\\torch\\optim\\optimizer.py:113\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m    112\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 113\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\JaceL\\miniconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\JaceL\\miniconda3\\lib\\site-packages\\torch\\optim\\adam.py:157\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    153\u001b[0m                 max_exp_avg_sqs\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mmax_exp_avg_sq\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m    155\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m--> 157\u001b[0m     adam(params_with_grad,\n\u001b[0;32m    158\u001b[0m          grads,\n\u001b[0;32m    159\u001b[0m          exp_avgs,\n\u001b[0;32m    160\u001b[0m          exp_avg_sqs,\n\u001b[0;32m    161\u001b[0m          max_exp_avg_sqs,\n\u001b[0;32m    162\u001b[0m          state_steps,\n\u001b[0;32m    163\u001b[0m          amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    164\u001b[0m          beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    165\u001b[0m          beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    166\u001b[0m          lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    167\u001b[0m          weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    168\u001b[0m          eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    169\u001b[0m          maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    170\u001b[0m          foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    171\u001b[0m          capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m    173\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\JaceL\\miniconda3\\lib\\site-packages\\torch\\optim\\adam.py:213\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 213\u001b[0m func(params,\n\u001b[0;32m    214\u001b[0m      grads,\n\u001b[0;32m    215\u001b[0m      exp_avgs,\n\u001b[0;32m    216\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    217\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    218\u001b[0m      state_steps,\n\u001b[0;32m    219\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    220\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    221\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    222\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    223\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    224\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    225\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    226\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable)\n",
      "File \u001b[1;32mc:\\Users\\JaceL\\miniconda3\\lib\\site-packages\\torch\\optim\\adam.py:256\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[0;32m    253\u001b[0m     \u001b[39massert\u001b[39;00m param\u001b[39m.\u001b[39mis_cuda \u001b[39mand\u001b[39;00m step_t\u001b[39m.\u001b[39mis_cuda, \u001b[39m\"\u001b[39m\u001b[39mIf capturable=True, params and state_steps must be CUDA tensors.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    255\u001b[0m \u001b[39m# update step\u001b[39;00m\n\u001b[1;32m--> 256\u001b[0m step_t \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    258\u001b[0m \u001b[39mif\u001b[39;00m weight_decay \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    259\u001b[0m     grad \u001b[39m=\u001b[39m grad\u001b[39m.\u001b[39madd(param, alpha\u001b[39m=\u001b[39mweight_decay)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "num_episodes = 1000\n",
    "batch_size = 64\n",
    "\n",
    "env = gym.make('CartPole-v1', new_step_api=True)\n",
    "\n",
    "# generate a mlp module\n",
    "net = generate_mlpnet(mlp_layers=[env.observation_space.shape[0], 128, env.action_space.n ])\n",
    "\n",
    "# set a replaybuffer\n",
    "replaybuffer = ReplayBuffer(1000)\n",
    "\n",
    "# set a dqn agent\n",
    "dqnagent = DQN(\n",
    "    net = net, \n",
    "    loss = F.mse_loss,\n",
    "    nums_action = env.action_space.n\n",
    "    )\n",
    "\n",
    "train = DqnTrain(\n",
    "    env=env,\n",
    "    agent=dqnagent,\n",
    "    replaybuffer=replaybuffer,\n",
    "    batchsize=batch_size,\n",
    "    tblogpath='./tensorlog'\n",
    "    )\n",
    "\n",
    "train.train(num_episodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqnagent.save_net(dqnagent.q_net ,'./model/dqnCartpole.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=./tensorlog --port 8123\n",
    "# open web browser and visit 127.0.0.1:8123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v1', new_step_api=True,render_mode='human')\n",
    "\n",
    "net = generate_mlpnet(mlp_layers=[env.observation_space.shape[0], 128, env.action_space.n ])\n",
    "model = torch.load('./model/dqnCartpole.pt')\n",
    "net.load_state_dict(model)\n",
    "\n",
    "done = False\n",
    "state = env.reset()\n",
    "while not done:\n",
    "    env.render()\n",
    "    state = torch.tensor(np.array([state]), dtype=torch.float).to(get_device())\n",
    "    action = net(state).argmax().item()\n",
    "    got = env.step(action=action)\n",
    "    state = got[0]\n",
    "    done = got[2]\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b9f5ab073a657fe4cc5417b6d00e97d396a1575f5354cbd9223aaad774df1bf7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
